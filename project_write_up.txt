Text Mining Mini-Project 3

Bryce Mann

OVERVIEW
In this project I tried to make a program that lets people input a link to a text file that they want to compare (by cosine similarity of the works' word frequencies) to the others in the program, saves that file to the folder, then allows them to input files in the folder to the similarity calculator. The similarity was calculated using the sklearn package, and the final output of the program is a plot where the relative similarity between text files determines how far away they are from each other.

IMPLEMENTATION
I started this project with one major goal in mind, and that was to make something that involved user input, and not manually changing the code everytime I wanted to add to the analysis. The program starts with asking the user to paste in a link to a text file that they want to add to the analysis (as of right now it handles Project Gutenberg best by default). It then asks the user what they want to save the file as, and saves that file in the folder. When the user is ready to analyze the texts in the folder, they have to enter in the name of the text they want to add, the label they want to give it in the plot, and whether or not they want to eliminate common English words from the analysis. There are also optional inputs where you can specify at what lines in the text the analysis should begin and end, but the user needs to be very specific with those inputs and they are not necessary with Gutenberg texts. Once this is entered the program will run and output a similarity plot of all of the texts stored in the database that the user can enjoy.

In terms of what is happening inside the program, saving the specified text is pretty easy; it uses the requests package to extract the link and saves the file as whatever the user inputs. When the analysis runs it takes the text file and turns it into a string that does not include any digits, uppercase letters, or punctuation. All of the whitespaces except for " " are eliminated as well. At this point the string (which was the downloaded text file) and the user specified label are stored in separate files: the strings for each text are stored in a list in a file called 'texts_to_analyze', and the labels are stored in a dictionary (where the value is the label, and the key is the corresponding text file's list index in 'texts_to_analyze') in a file called 'labels'. Once this information is saved, I used parts of the sklearn package to vectorize the strings in the list and compute the cosine similarity matrix of all of the texts. This matrix is then plotted and displayed.

The best decision I made while making this was taking choices away from the user. At first I was trying to make it as customizable as possible, but realized that having the user specify the names of the files that they wanted to save the data structures in and having no default values for the beginning and end strings made it very hard to safely add anything without a typo. I ended up completely removing the option to specify the data structure file names and did that automatically, and added default values for the beginning and end strings to work with Gutenberg files. As soon as I did that it worked how I wanted and I realized that when it comes to programs people should make as few decisions as possible. With more time to refine I could probably automate more of the program, but I am happy with where it is right now.

RESULTS
The results of my analysis with 10 books, with and without the most common English words, are stored as .pngs in my repository. I picked a lot of famous books spanning thousands of years so there is a relatively even distribution across the plot. With only 10 entries so far, it is not too interesting at the moment, but as more and more sources get added the clusters will get more pronounced and it will be easy to keep adding and see where different sources get placed relative to each other.

REFLECTION
I think I did a pretty good job of laying out my program and building it incrementally. My next steps to improve it would be to make the final output plot look nicer by adding a title that describes what went into that specific plot and to try to figure out an easier way for it to handle non-Gutenberg texts, because as of right now you need to be very specific and exact when entering in your start and stop strings to get it to do what you want. I would also want to figure out a better way to test the functions, because the way it was set up every function was either handling a large amount of data or doing something very complex so my way of testing it was to run it with inputs and fix errors as they showed up, which was not the most efficient or best way to go about it. Moving forward, I am definitely going to remember that the less arguments that you have to pass from function to function, especially when they originate from user input, the better. Once I figured that out and took away a lot of the decisions that the users had to make the program was much easier to use and debug.
